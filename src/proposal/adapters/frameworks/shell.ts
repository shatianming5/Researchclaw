export function renderDiscoverPlanDirCommand(): string {
  return [
    "openclaw_find_plan_dir() {",
    '  d="$PWD"',
    '  while [ "$d" != "/" ]; do',
    '    if [ -f "$d/plan/plan.dag.json" ] && [ -f "$d/report/compile_report.json" ]; then',
    '      echo "$d"',
    "      return",
    "    fi",
    '    d="$(dirname "$d")"',
    "  done",
    '  echo ""',
    "}",
    'OPENCLAW_PLAN_DIR="$(openclaw_find_plan_dir)"',
    'if [ -z "$OPENCLAW_PLAN_DIR" ]; then',
    '  echo "Could not locate OpenClaw plan root (missing plan/plan.dag.json)" >&2',
    "  exit 1",
    "fi",
  ].join("\n");
}

export function renderExportStandardCacheEnvCommand(): string {
  return [
    'export HF_HOME="$OPENCLAW_PLAN_DIR/cache/hf"',
    'export TRANSFORMERS_CACHE="$OPENCLAW_PLAN_DIR/cache/hf/transformers"',
    'export HF_DATASETS_CACHE="$OPENCLAW_PLAN_DIR/cache/hf/datasets"',
    'export PIP_CACHE_DIR="$OPENCLAW_PLAN_DIR/cache/pip"',
  ].join("\n");
}

export function renderWritePipFreezeCommand(params: { repoKey: string }): string[] {
  const repoKey = params.repoKey.trim();
  return [
    `mkdir -p "$OPENCLAW_PLAN_DIR/plan/locks/${repoKey}"`,
    `python -m pip freeze > "$OPENCLAW_PLAN_DIR/plan/locks/${repoKey}/pip-freeze.txt"`,
    `echo "$OPENCLAW_PLAN_DIR/plan/locks/${repoKey}/pip-freeze.txt"`,
  ];
}

export function renderWriteEvalMetricsCommand(params: {
  metrics?: Record<string, unknown>;
  notes?: string[];
}): string {
  const metricsJson = JSON.stringify(params.metrics ?? {});
  const notesJson = JSON.stringify(params.notes ?? []);
  const metricsB64 = Buffer.from(metricsJson, "utf-8").toString("base64");
  const notesB64 = Buffer.from(notesJson, "utf-8").toString("base64");
  return [
    "python3 - <<'PY'",
    "import base64, datetime, json, os, pathlib",
    "plan_dir = pathlib.Path(os.environ.get('OPENCLAW_PLAN_DIR', '.')).resolve()",
    "report_dir = plan_dir / 'report'",
    "report_dir.mkdir(parents=True, exist_ok=True)",
    `metrics = json.loads(base64.b64decode('${metricsB64}').decode('utf-8'))`,
    `notes = json.loads(base64.b64decode('${notesB64}').decode('utf-8'))`,
    "payload = {",
    "  'schemaVersion': 1,",
    "  'createdAt': datetime.datetime.utcnow().isoformat() + 'Z',",
    "  'metrics': metrics,",
    "  'notes': notes,",
    "}",
    "(report_dir / 'eval_metrics.json').write_text(json.dumps(payload, indent=2, ensure_ascii=False) + '\\n', encoding='utf-8')",
    "print(str(report_dir / 'eval_metrics.json'))",
    "PY",
  ].join("\n");
}

export function renderExtractLightningEvalMetricsCommand(params?: {
  outputDirEnv?: string;
}): string {
  const outputDirEnv = params?.outputDirEnv?.trim() || "OPENCLAW_OUTPUT_DIR";
  return [
    "python3 - <<'PY'",
    "import csv, datetime, json, os, pathlib",
    "plan_dir = pathlib.Path(os.environ.get('OPENCLAW_PLAN_DIR', '.')).resolve()",
    `output_dir = pathlib.Path(os.environ.get('${outputDirEnv}', str(plan_dir))).resolve()`,
    "report_dir = plan_dir / 'report'",
    "report_dir.mkdir(parents=True, exist_ok=True)",
    "notes = []",
    "metrics = {}",
    "def rel(p: pathlib.Path) -> str:",
    "  try:",
    "    return str(p.relative_to(plan_dir))",
    "  except Exception:",
    "    return str(p)",
    "candidates = sorted(set(output_dir.rglob('metrics.csv')), key=lambda p: p.stat().st_mtime if p.exists() else 0)",
    "if not candidates:",
    "  notes.append('Lightning: no metrics.csv found under output dir: ' + rel(output_dir))",
    "else:",
    "  selected = candidates[-1]",
    "  notes.append('Lightning: selected metrics CSV: ' + rel(selected))",
    "  last = None",
    "  try:",
    "    with selected.open('r', encoding='utf-8', errors='ignore', newline='') as f:",
    "      reader = csv.DictReader(f)",
    "      for row in reader:",
    "        if row:",
    "          last = row",
    "  except Exception as e:",
    "    notes.append('Lightning: failed reading CSV: ' + str(e))",
    "    last = None",
    "  if last:",
    "    for k, v in last.items():",
    "      if not k or k in ('step', 'epoch') or v is None:",
    "        continue",
    "      s = str(v).strip()",
    "      if not s:",
    "        continue",
    "      try:",
    "        num = float(s)",
    "      except Exception:",
    "        continue",
    "      if num != num:",
    "        continue",
    "      metrics[str(k).strip()] = num",
    "payload = {",
    "  'schemaVersion': 1,",
    "  'createdAt': datetime.datetime.utcnow().isoformat() + 'Z',",
    "  'metrics': metrics,",
    "  'notes': notes,",
    "}",
    "(report_dir / 'eval_metrics.json').write_text(json.dumps(payload, indent=2, ensure_ascii=False) + '\\n', encoding='utf-8')",
    "print(str(report_dir / 'eval_metrics.json'))",
    "PY",
  ].join("\n");
}

export function renderExtractMmengineEvalMetricsCommand(params?: {
  outputDirEnv?: string;
}): string {
  const outputDirEnv = params?.outputDirEnv?.trim() || "OPENCLAW_OUTPUT_DIR";
  return [
    "python3 - <<'PY'",
    "import datetime, json, os, pathlib, re",
    "plan_dir = pathlib.Path(os.environ.get('OPENCLAW_PLAN_DIR', '.')).resolve()",
    `output_dir = pathlib.Path(os.environ.get('${outputDirEnv}', str(plan_dir))).resolve()`,
    "report_dir = plan_dir / 'report'",
    "report_dir.mkdir(parents=True, exist_ok=True)",
    "notes = []",
    "metrics = {}",
    "allow = re.compile(r'(mAP|AP|mIoU|acc|accuracy|AR|precision|recall|f1)', re.I)",
    "def rel(p: pathlib.Path) -> str:",
    "  try:",
    "    return str(p.relative_to(plan_dir))",
    "  except Exception:",
    "    return str(p)",
    "scalars_path = output_dir / 'vis_data' / 'scalars.json'",
    "if not scalars_path.exists():",
    "  notes.append('MMEngine: no vis_data/scalars.json found under output dir: ' + rel(output_dir))",
    "else:",
    "  try:",
    "    obj = json.loads(scalars_path.read_text(encoding='utf-8', errors='ignore'))",
    "    entries = obj if isinstance(obj, list) else obj.get('data') if isinstance(obj, dict) else None",
    "    if isinstance(entries, list):",
    "      best = {}",
    "      idx = 0",
    "      for e in entries:",
    "        idx += 1",
    "        if not isinstance(e, dict):",
    "          continue",
    "        tag = e.get('tag') or e.get('name') or e.get('key')",
    "        value = e.get('value')",
    "        step = e.get('step')",
    "        if tag is None or value is None:",
    "          continue",
    "        tag = str(tag)",
    "        if not allow.search(tag):",
    "          continue",
    "        try:",
    "          num = float(value)",
    "        except Exception:",
    "          continue",
    "        if num != num:",
    "          continue",
    "        step_key = step if isinstance(step, (int, float)) else idx",
    "        prev = best.get(tag)",
    "        if prev is None or step_key >= prev[0]:",
    "          best[tag] = (step_key, num)",
    "      for k, (_, v) in best.items():",
    "        metrics[k] = v",
    "      notes.append(f'MMEngine: extracted {len(best)} metrics from ' + rel(scalars_path))",
    "    else:",
    "      notes.append('MMEngine: scalars.json present but unexpected structure: ' + rel(scalars_path))",
    "  except Exception as e:",
    "    notes.append('MMEngine: failed parsing scalars.json: ' + str(e))",
    "payload = {",
    "  'schemaVersion': 1,",
    "  'createdAt': datetime.datetime.utcnow().isoformat() + 'Z',",
    "  'metrics': metrics,",
    "  'notes': notes,",
    "}",
    "(report_dir / 'eval_metrics.json').write_text(json.dumps(payload, indent=2, ensure_ascii=False) + '\\n', encoding='utf-8')",
    "print(str(report_dir / 'eval_metrics.json'))",
    "PY",
  ].join("\n");
}

export function renderExtractTransformersEvalMetricsCommand(params?: {
  outputDirEnv?: string;
}): string {
  const outputDirEnv = params?.outputDirEnv?.trim() || "OPENCLAW_OUTPUT_DIR";
  return [
    "python3 - <<'PY'",
    "import datetime, json, os, pathlib",
    "plan_dir = pathlib.Path(os.environ.get('OPENCLAW_PLAN_DIR', '.')).resolve()",
    `output_dir = pathlib.Path(os.environ.get('${outputDirEnv}', str(plan_dir))).resolve()`,
    "report_dir = plan_dir / 'report'",
    "report_dir.mkdir(parents=True, exist_ok=True)",
    "notes = []",
    "metrics = {}",
    "def rel(p: pathlib.Path) -> str:",
    "  try:",
    "    return str(p.relative_to(plan_dir))",
    "  except Exception:",
    "    return str(p)",
    "def as_num(v):",
    "  try:",
    "    num = float(v)",
    "  except Exception:",
    "    return None",
    "  if num != num:",
    "    return None",
    "  return num",
    "def parse_metrics(obj):",
    "  if not isinstance(obj, dict):",
    "    return {}",
    "  out = {}",
    "  for k, v in obj.items():",
    "    if not isinstance(k, str):",
    "      continue",
    "    num = as_num(v)",
    "    if num is None:",
    "      continue",
    "    out[k] = num",
    "  return out",
    "candidates = []",
    "for name in ('all_results.json','eval_results.json','test_results.json','train_results.json','results.json','metrics.json'):",
    "  candidates += list(output_dir.rglob(name))",
    "candidates = [p for p in candidates if p.is_file()]",
    "candidates = sorted(set(candidates), key=lambda p: p.stat().st_mtime if p.exists() else 0)",
    "for p in reversed(candidates):",
    "  try:",
    "    obj = json.loads(p.read_text(encoding='utf-8', errors='ignore'))",
    "  except Exception:",
    "    continue",
    "  parsed = parse_metrics(obj)",
    "  if parsed:",
    "    metrics = parsed",
    "    notes.append('Transformers: selected metrics file: ' + rel(p))",
    "    break",
    "if metrics:",
    "  preferred = {k: v for k, v in metrics.items() if isinstance(k, str) and k.startswith('eval_')}",
    "  if preferred:",
    "    metrics = preferred",
    "    notes.append('Transformers: kept eval_* metrics only')",
    "if not metrics:",
    "  state_candidates = sorted(set([p for p in output_dir.rglob('trainer_state.json') if p.is_file()]), key=lambda p: p.stat().st_mtime if p.exists() else 0)",
    "  if state_candidates:",
    "    p = state_candidates[-1]",
    "    notes.append('Transformers: parsing trainer_state.json: ' + rel(p))",
    "    try:",
    "      obj = json.loads(p.read_text(encoding='utf-8', errors='ignore'))",
    "      log = obj.get('log_history') if isinstance(obj, dict) else None",
    "      if isinstance(log, list):",
    "        for entry in reversed(log):",
    "          if not isinstance(entry, dict):",
    "            continue",
    "          parsed = parse_metrics(entry)",
    "          if parsed:",
    "            metrics = parsed",
    "            break",
    "    except Exception as e:",
    "      notes.append('Transformers: failed parsing trainer_state.json: ' + str(e))",
    "    if metrics:",
    "      preferred = {k: v for k, v in metrics.items() if isinstance(k, str) and k.startswith('eval_')}",
    "      if preferred:",
    "        metrics = preferred",
    "        notes.append('Transformers: kept eval_* metrics only')",
    "  else:",
    "    notes.append('Transformers: no metrics candidates found under output dir: ' + rel(output_dir))",
    "payload = {",
    "  'schemaVersion': 1,",
    "  'createdAt': datetime.datetime.utcnow().isoformat() + 'Z',",
    "  'metrics': metrics,",
    "  'notes': notes,",
    "}",
    "(report_dir / 'eval_metrics.json').write_text(json.dumps(payload, indent=2, ensure_ascii=False) + '\\n', encoding='utf-8')",
    "print(str(report_dir / 'eval_metrics.json'))",
    "PY",
  ].join("\n");
}

export function renderExtractDetectron2EvalMetricsCommand(params?: {
  outputDirEnv?: string;
}): string {
  const outputDirEnv = params?.outputDirEnv?.trim() || "OPENCLAW_OUTPUT_DIR";
  return [
    "python3 - <<'PY'",
    "import datetime, json, os, pathlib, re",
    "plan_dir = pathlib.Path(os.environ.get('OPENCLAW_PLAN_DIR', '.')).resolve()",
    `output_dir = pathlib.Path(os.environ.get('${outputDirEnv}', str(plan_dir))).resolve()`,
    "report_dir = plan_dir / 'report'",
    "report_dir.mkdir(parents=True, exist_ok=True)",
    "notes = []",
    "metrics = {}",
    "allow = re.compile(r'(?:^|/)(AP|AP50|AP75|mAP)(?:$|[^A-Za-z0-9])', re.I)",
    "def rel(p: pathlib.Path) -> str:",
    "  try:",
    "    return str(p.relative_to(plan_dir))",
    "  except Exception:",
    "    return str(p)",
    "metrics_path = output_dir / 'metrics.json'",
    "if not metrics_path.exists():",
    "  notes.append('Detectron2: no metrics.json found under output dir: ' + rel(output_dir))",
    "else:",
    "  try:",
    "    lines = metrics_path.read_text(encoding='utf-8', errors='ignore').splitlines()",
    "    tail = lines[-5000:] if len(lines) > 5000 else lines",
    "    for line in reversed(tail):",
    "      line = line.strip()",
    "      if not line:",
    "        continue",
    "      try:",
    "        obj = json.loads(line)",
    "      except Exception:",
    "        continue",
    "      if not isinstance(obj, dict):",
    "        continue",
    "      extracted = {}",
    "      for k, v in obj.items():",
    "        if not isinstance(k, str) or not allow.search(k):",
    "          continue",
    "        try:",
    "          num = float(v)",
    "        except Exception:",
    "          continue",
    "        if num != num:",
    "          continue",
    "        extracted[k] = num",
    "      if extracted:",
    "        metrics = extracted",
    "        notes.append('Detectron2: extracted metrics from ' + rel(metrics_path))",
    "        break",
    "    if not metrics:",
    "      notes.append('Detectron2: metrics.json present but no AP-like keys found: ' + rel(metrics_path))",
    "  except Exception as e:",
    "    notes.append('Detectron2: failed parsing metrics.json: ' + str(e))",
    "payload = {",
    "  'schemaVersion': 1,",
    "  'createdAt': datetime.datetime.utcnow().isoformat() + 'Z',",
    "  'metrics': metrics,",
    "  'notes': notes,",
    "}",
    "(report_dir / 'eval_metrics.json').write_text(json.dumps(payload, indent=2, ensure_ascii=False) + '\\n', encoding='utf-8')",
    "print(str(report_dir / 'eval_metrics.json'))",
    "PY",
  ].join("\n");
}
